{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\"conf\": {\"spark.jars.packages\": \"org.postgresql:postgresql:42.2.5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>22</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s test -l scala -u http://192.168.1.114:8998/ -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Julia interpreter. This may take some time...\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import julia.magic\n",
    "julia.magic.load_ipython_extension(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "ip = get_ipython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Map[String,String] = Map(spark.yarn.dist.archives -> file:/opt/apache-spark/R/lib/sparkr.zip#sparkr, spark.driver.host -> localhost, spark.livy.spark_major_version -> 2, spark.driver.port -> 39303, spark.repl.class.uri -> spark://localhost:39303/classes, spark.jars -> file:///opt/apache-livy/rsc-jars/livy-api-0.5.0-incubating.jar,file:///opt/apache-livy/rsc-jars/netty-all-4.0.37.Final.jar,file:///opt/apache-livy/rsc-jars/livy-rsc-0.5.0-incubating.jar,file:///opt/apache-livy/repl_2.11-jars/commons-codec-1.9.jar,file:///opt/apache-livy/repl_2.11-jars/livy-core_2.11-0.5.0-incubating.jar,file:///opt/apache-livy/repl_2.11-jars/livy-repl_2.11-0.5.0-incubating.jar,file:///var/lib/apache-livy/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar, spark.repl.class.outputDir -> /tmp/spark88490917..."
     ]
    }
   ],
   "source": [
    "spark.conf.getAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined object spark_read_db\n"
     ]
    }
   ],
   "source": [
    "object spark_read_db {\n",
    "    val opts = Map(\"url\" -> \"jdbc:postgresql://localhost:5733/j3\", \"driver\" -> \"org.postgresql.Driver\")\n",
    "    def table(dbtable:String) =\n",
    "        spark.read.format(\"jdbc\").options(opts + (\"dbtable\" -> dbtable))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items: org.apache.spark.sql.DataFrame = [tag: string, id: string ... 3 more fields]\n",
      "root\n",
      " |-- tag: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|         tag|\n",
      "+------------+\n",
      "|       equip|\n",
      "|metric_names|\n",
      "|      talent|\n",
      "|  attr_names|\n",
      "|     version|\n",
      "|      kungfu|\n",
      "+------------+\n",
      "\n",
      "matches: org.apache.spark.sql.DataFrame = [match_id: bigint, start_time: int ... 11 more fields]\n",
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- start_time: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- pvp_type: integer (nullable = true)\n",
      " |-- map: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- team1_score: integer (nullable = true)\n",
      " |-- team2_score: integer (nullable = true)\n",
      " |-- team1_kungfu: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- team2_kungfu: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- winner: integer (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- role_ids: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "scores: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 13 more fields]\n",
      "root\n",
      " |-- role_id: string (nullable = true)\n",
      " |-- match_type: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- score2: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- ranking: integer (nullable = true)\n",
      " |-- ranking2: integer (nullable = true)\n",
      " |-- total_count: integer (nullable = true)\n",
      " |-- win_count: integer (nullable = true)\n",
      " |-- mvp_count: integer (nullable = true)\n",
      " |-- fetched_at: timestamp (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- fetched_count: integer (nullable = true)\n",
      " |-- fetched_to: integer (nullable = true)\n",
      "\n",
      "role_kungfus: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 8 more fields]\n",
      "root\n",
      " |-- role_id: string (nullable = true)\n",
      " |-- match_type: string (nullable = true)\n",
      " |-- kungfu: string (nullable = true)\n",
      " |-- mvp_count: integer (nullable = true)\n",
      " |-- total_count: integer (nullable = true)\n",
      " |-- win_count: integer (nullable = true)\n",
      " |-- metrics: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n",
      "roles: org.apache.spark.sql.DataFrame = [role_id: int, global_id: string ... 10 more fields]\n",
      "root\n",
      " |-- role_id: integer (nullable = true)\n",
      " |-- global_id: string (nullable = true)\n",
      " |-- passport_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- force: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- camp: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- server: string (nullable = true)\n",
      " |-- person_id: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n",
      "match_roles: org.apache.spark.sql.DataFrame = [match_id: bigint, role_id: string ... 14 more fields]\n",
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- role_id: string (nullable = true)\n",
      " |-- kungfu: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- score2: integer (nullable = true)\n",
      " |-- ranking: integer (nullable = true)\n",
      " |-- equip_score: integer (nullable = true)\n",
      " |-- equip_addition_score: integer (nullable = true)\n",
      " |-- max_hp: integer (nullable = true)\n",
      " |-- metrics_version: integer (nullable = true)\n",
      " |-- metrics: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- equips: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- talents: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- attrs: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- attrs_version: integer (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val items = spark_read_db.table(\"items\").load\n",
    "items.printSchema\n",
    "items.select(\"tag\").distinct().show()\n",
    "val matches = spark_read_db.table(\"match_3c.matches\").load\n",
    "matches.printSchema\n",
    "val scores = spark_read_db.table(\"scores\").load\n",
    "scores.printSchema\n",
    "val role_kungfus = spark_read_db.table(\"role_kungfus\").load\n",
    "role_kungfus.printSchema\n",
    "val roles = spark_read_db.table(\"roles\").load\n",
    "roles.printSchema\n",
    "val match_roles = spark_read_db.table(\"match_3c.match_roles\").load\n",
    "match_roles.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [grade: int, count: bigint ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o df\n",
    "val df = matches.groupBy(\"grade\").\n",
    "  agg(\n",
    "    count(\"match_id\").alias(\"count\"),\n",
    "    bround(avg(\"duration\"), 2).alias(\"duration\")).\n",
    "  sort(\"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111701e74d164b9a9ad79b38d74c8228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fcd3631d646dcb24611ffa6901f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kungfu_cn_map: scala.collection.immutable.Map[String,(String, String)] = Map(jingyu -> (惊羽,鲸), yunshang -> (云裳,秀), tiegu -> (铁骨,骨), lijing -> (离经,离), xisui -> (洗髓,洗), fenying -> (焚影,明), xiangzhi -> (相知,歌), taixu -> (太虚,剑), yijin -> (易经,秃), mingzun -> (明尊,喵), cangjian -> (藏剑,藏), tielao -> (铁牢,铁), aoxue -> (傲血,策), fenshan -> (分山,苍), mowen -> (莫问,莫), xiaochen -> (笑尘,丐), butian -> (补天,补), zixia -> (紫霞,气), beiao -> (北傲,霸), bingxin -> (冰心,冰), dujing -> (毒经,毒), tianluo -> (天罗,螺), huajian -> (花间,花))\n",
      "kungfu_items: org.apache.spark.sql.DataFrame = [content: string, id: int ... 2 more fields]\n",
      "+--------+-----+------+-----+\n",
      "| content|   id|kungfu|short|\n",
      "+--------+-----+------+-----+\n",
      "|   xisui|10002|    洗髓|    洗|\n",
      "|   yijin|10003|    易经|    秃|\n",
      "|   zixia|10014|    紫霞|    气|\n",
      "|   taixu|10015|    太虚|    剑|\n",
      "| huajian|10021|    花间|    花|\n",
      "|   aoxue|10026|    傲血|    策|\n",
      "|  lijing|10028|    离经|    离|\n",
      "|  tielao|10062|    铁牢|    铁|\n",
      "|yunshang|10080|    云裳|    秀|\n",
      "| bingxin|10081|    冰心|    冰|\n",
      "|cangjian|10145|    藏剑|    藏|\n",
      "|  dujing|10175|    毒经|    毒|\n",
      "|  butian|10176|    补天|    补|\n",
      "|  jingyu|10224|    惊羽|    鲸|\n",
      "| tianluo|10225|    天罗|    螺|\n",
      "| fenying|10242|    焚影|    明|\n",
      "| mingzun|10243|    明尊|    喵|\n",
      "|xiaochen|10268|    笑尘|    丐|\n",
      "|   tiegu|10389|    铁骨|    骨|\n",
      "| fenshan|10390|    分山|    苍|\n",
      "|   mowen|10447|    莫问|    莫|\n",
      "|xiangzhi|10448|    相知|    歌|\n",
      "|   beiao|10464|    北傲|    霸|\n",
      "+--------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark -o kungfu_items\n",
    "val kungfu_cn_map = Map(\n",
    "    \"xisui\" -> (\"洗髓\", \"洗\"),\n",
    "    \"yijin\" -> (\"易经\", \"秃\"),\n",
    "    \"zixia\" -> (\"紫霞\", \"气\"),\n",
    "    \"taixu\" -> (\"太虚\", \"剑\"),\n",
    "    \"huajian\" -> (\"花间\", \"花\"),\n",
    "    \"aoxue\" -> (\"傲血\", \"策\"),\n",
    "    \"lijing\" -> (\"离经\", \"离\"),\n",
    "    \"tielao\" -> (\"铁牢\", \"铁\"),\n",
    "    \"yunshang\" -> (\"云裳\", \"秀\"),\n",
    "    \"bingxin\" -> (\"冰心\", \"冰\"),\n",
    "    \"cangjian\" -> (\"藏剑\", \"藏\"),\n",
    "    \"dujing\" -> (\"毒经\", \"毒\"),\n",
    "    \"butian\" -> (\"补天\", \"补\"),\n",
    "    \"jingyu\" -> (\"惊羽\", \"鲸\"),\n",
    "    \"tianluo\" -> (\"天罗\", \"螺\"),\n",
    "    \"fenying\" -> (\"焚影\", \"明\"),\n",
    "    \"mingzun\" -> (\"明尊\", \"喵\"),\n",
    "    \"xiaochen\" -> (\"笑尘\", \"丐\"),\n",
    "    \"tiegu\" -> (\"铁骨\", \"骨\"),\n",
    "    \"fenshan\" -> (\"分山\", \"苍\"),\n",
    "    \"mowen\" -> (\"莫问\", \"莫\"),\n",
    "    \"xiangzhi\" -> (\"相知\", \"歌\"),\n",
    "    \"beiao\" -> (\"北傲\", \"霸\")\n",
    ")\n",
    "val kungfu_items = items.filter(\"tag == 'kungfu'\").\n",
    "    select($\"id\".cast(\"int\"), trim($\"content\", \"\\\"\").alias(\"content\")).\n",
    "    join(kungfu_cn_map.toList.map { case (a, (b, c)) => (a,b,c) }.toDF(\"content\", \"kungfu\", \"short\"), Seq(\"content\"), \"left_outer\")\n",
    "kungfu_items.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.DataFrame\n",
      "import scala.collection.JavaConverters._\n",
      "get_kungfus: (matches: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "defined object kungfu_sort_fun\n",
      "show_kungfu: (kungfus: org.apache.spark.sql.DataFrame, n: Int)scala.collection.mutable.Buffer[(String, Long, Long, Float)]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.collection.JavaConverters._\n",
    "def get_kungfus(matches:DataFrame) = {\n",
    "    matches.selectExpr(\"team1_kungfu as kungfu\", \"winner=1 as won\").\n",
    "        union(matches.selectExpr(\"team2_kungfu as kungfu\", \"winner=2 as won\"))\n",
    "}\n",
    "object kungfu_sort_fun {\n",
    "    val kungfu_order = Seq(\n",
    "        \"taixu\",\n",
    "        \"zixia\", \"bingxin\", \"fenying\", \"yijin\", \"huajian\", \"dujing\", \"tianluo\",  \"mowen\",\n",
    "        \"aoxue\", \"fenshan\", \"cangjian\", \"jingyu\", \"xiaochen\", \"beiao\",\n",
    "        \"xisui\", \"tielao\", \"tiegu\", \"mingzun\",\n",
    "        \"lijing\", \"yunshang\", \"butian\", \"xiangzhi\"\n",
    "    ).zipWithIndex.toMap\n",
    "    val kungfu_order_sp = Set((\"jingyu\", \"tianluo\"))\n",
    "    def call(a:String, b:String) : Boolean = {\n",
    "        if (kungfu_order_sp.contains((a, b))) {\n",
    "            return true\n",
    "        }\n",
    "        if (kungfu_order_sp.contains((b, a))) {\n",
    "            return false\n",
    "        }\n",
    "        return kungfu_order(a) < kungfu_order(b)\n",
    "    }\n",
    "}\n",
    "def show_kungfu(kungfus:DataFrame, n:Int) = {\n",
    "    val kungfu_map = kungfu_items.select(\"id\", \"content\").takeAsList(100).asScala.map(i => (i.getAs[Int](0), i.getAs[String](1))).toMap\n",
    "    val kungfu_short_map = kungfu_items.select(\"content\", \"short\").takeAsList(100).asScala.map(i => (i.getAs[String](0), i.getAs[String](1))).toMap\n",
    "    kungfus.takeAsList(n).asScala.map(\n",
    "        i => (\n",
    "            i.getAs[Seq[Int]](0).map(kungfu_map).sortWith(kungfu_sort_fun.call).map(kungfu_short_map).mkString,\n",
    "            i.getAs[Long](1)+1,\n",
    "            i.getAs[Long](2),\n",
    "            i.getAs[Long](2).toFloat/i.getAs[Long](1)\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kungfu_sort: (grade: Int)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "kungfu_sort_12_result: scala.collection.mutable.Buffer[(String, Long, Long, Float)] = ArrayBuffer((策藏秀,73808,38945,0.52765995), (鲸丐秀,40488,23150,0.5717885), (剑藏秀,39843,22674,0.56909794), (气花离,36285,20414,0.5626171), (剑霸秀,32083,18247,0.5687613), (策藏补,32202,16019,0.497469), (毒毒补,26732,15320,0.5731174), (剑霸补,28847,15037,0.5212855), (策鲸秀,27111,14613,0.5390262), (明莫补,24493,14242,0.581496), (策霸秀,29065,14213,0.48902422), (策霸补,27228,13346,0.4901752), (气花补,20359,11201,0.5502014), (剑丐秀,21118,11180,0.5294313), (剑藏补,20990,10766,0.51293534), (秃螺补,17195,10226,0.59474236), (剑策秀,18069,9348,0.5173788), (苍霸补,17435,8990,0.51565903), (秃霸补,15332,8534,0.5566499), (策苍补,16664,8063,0.48388645), (策苍秀,16161,7826,0.48428217), (苍藏秀,15268,7637,0.50022924), (策鲸补,15821,7460,0.471555), (冰毒补,13250,7357,0.5552872), (毒莫补,...kungfu_sort_13_result: scala.collection.mutable.Buffer[(String, Long, Long, Float)] = ArrayBuffer((鲸丐秀,18952,10080,0.5318981), (策藏秀,19349,9619,0.49715734), (剑藏秀,16924,9004,0.532057), (毒毒补,13928,8064,0.5790192), (剑霸秀,14097,7614,0.5401532), (气花离,11197,6024,0.5380493), (秃螺补,10233,5977,0.58414775), (明莫补,9029,5077,0.56236154), (秃霸补,6936,3863,0.55702955), (剑霸补,7498,3769,0.5027344), (策鲸秀,7499,3498,0.4665244), (剑丐秀,6973,3470,0.4977051), (毒螺补,5013,2740,0.54668796), (冰莫歌,3877,2406,0.62074304), (冰鲸秀,4052,2357,0.58183163), (气花补,4421,2318,0.5244344), (冰毒补,4097,2230,0.5444336), (明莫歌,3748,2086,0.55671203), (苍霸补,3617,1982,0.5481195), (毒莫补,3747,1969,0.5256273), (剑藏补,4143,1884,0.45485273), (气花秀,3257,1816,0.55773956), (剑策秀,3750,1762,0.469992), (气鲸离,3186,1739,0.54599684), (秃秃补,2875,1646,0.57272094), (秃螺秀,2..."
     ]
    }
   ],
   "source": [
    "def kungfu_sort(grade:Int) =\n",
    "    get_kungfus(matches.filter($\"grade\" === grade)).groupBy(\"kungfu\").\n",
    "    agg(\n",
    "        count(lit(1)).alias(\"count\"),\n",
    "        count(when($\"won\" === true, true)).alias(\"won\")\n",
    "    ).sort($\"won\".desc);\n",
    "val kungfu_sort_12_result = show_kungfu(kungfu_sort(12), 1000)\n",
    "val kungfu_sort_13_result = show_kungfu(kungfu_sort(13), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(策藏秀,19332,9611,0.4971807)\n",
      "(鲸丐秀,18946,10076,0.53185534)\n",
      "(剑藏秀,16919,9002,0.53209597)\n",
      "(剑霸秀,14089,7610,0.54017603)\n",
      "(毒毒补,13927,8064,0.57906073)\n",
      "(气花离,11196,6023,0.53800803)\n",
      "(秃螺补,10226,5972,0.5840587)\n",
      "(明莫补,9029,5077,0.56236154)\n",
      "(策鲸秀,7494,3495,0.46643534)\n",
      "(剑霸补,7493,3765,0.50253606)\n",
      "(剑丐秀,6971,3469,0.49770445)\n",
      "(秃霸补,6933,3862,0.55712634)\n",
      "(毒螺补,5013,2740,0.54668796)\n",
      "(气花补,4420,2318,0.52455306)\n",
      "(剑藏补,4136,1879,0.45441353)\n",
      "(冰毒补,4095,2229,0.5444553)\n",
      "(冰鲸秀,4052,2357,0.58183163)\n",
      "(冰莫歌,3864,2401,0.6215377)\n",
      "(毒莫补,3747,1969,0.5256273)\n",
      "(明莫歌,3746,2084,0.5564753)\n",
      "(剑策秀,3746,1759,0.46969292)\n",
      "(苍霸补,3616,1982,0.5482711)\n",
      "(气花秀,3257,1816,0.55773956)\n",
      "(气鲸离,3186,1739,0.54599684)\n",
      "(策藏补,2908,1283,0.44134846)\n",
      "(策霸秀,2893,1208,0.41770402)\n",
      "(秃秃补,2875,1646,0.57272094)\n",
      "(策霸补,2789,1290,0.46269727)\n",
      "(秃螺秀,2763,1482,0.5365677)\n",
      "(藏鲸秀,2759,1476,0.53517044)\n",
      "(冰鲸离,2698,1436,0.53244346)\n",
      "(剑气离,2547,1286,0.50510603)\n",
      "(苍藏秀,2375,1189,0.50084245)\n",
      "(气明秀,2197,1359,0.61885244)\n",
      "(秃霸秀,2156,1103,0.51183295)\n",
      "(苍霸秀,2117,1062,0.50189036)\n",
      "(剑丐补,1943,848,0.43666324)\n",
      "(明莫离,1897,985,0.51951474)\n",
      "(气鲸秀,1863,975,0.5236305)\n",
      "(剑策补,1857,766,0.41271552)\n",
      "(明莫秀,1830,1053,0.5757244)\n",
      "(气花歌,1744,1016,0.582903)\n",
      "(鲸螺秀,1701,927,0.5452941)\n",
      "(策鲸补,1639,720,0.43956044)\n",
      "(气明离,1596,834,0.522884)\n",
      "(冰毒秀,1545,959,0.621114)\n",
      "(冰莫补,1437,739,0.51462394)\n",
      "(剑螺秀,1424,733,0.51510894)\n",
      "(策苍补,1409,687,0.48792613)\n",
      "(气明补,1329,633,0.47665662)\n",
      "(冰明秀,1290,749,0.5810706)\n",
      "(冰冰秀,1225,743,0.60702616)\n",
      "(明毒补,1150,677,0.589208)\n",
      "(秃莫补,1117,525,0.4704301)\n",
      "(策苍秀,1116,435,0.39013454)\n",
      "(冰明离,1046,511,0.48899522)\n",
      "(气冰补,1019,497,0.48821217)\n",
      "(鲸丐补,993,355,0.3578629)\n",
      "(苍丐秀,942,418,0.4442083)\n",
      "(冰鲸补,940,381,0.4057508)\n",
      "(气冰离,926,408,0.44108108)\n",
      "(剑螺补,914,405,0.44359255)\n",
      "(毒毒秀,898,504,0.5618729)\n",
      "(剑苍秀,882,364,0.41316685)\n",
      "(剑螺离,880,417,0.47440273)\n",
      "(毒苍补,878,563,0.6419612)\n",
      "(剑苍补,870,412,0.47410816)\n",
      "(冰明补,856,391,0.45730993)\n",
      "(藏霸秀,845,370,0.43838862)\n",
      "(藏鲸补,820,372,0.45421246)\n",
      "(毒螺秀,809,482,0.59653467)\n",
      "(冰花秀,775,399,0.5155039)\n",
      "(鲸霸秀,775,386,0.498708)\n",
      "(策丐秀,769,289,0.3763021)\n",
      "(苍藏补,768,368,0.4797914)\n",
      "(冰花离,768,352,0.4589309)\n",
      "(冰莫秀,766,419,0.54771245)\n",
      "(明花离,754,384,0.5099602)\n",
      "(秃霸歌,728,366,0.5034388)\n",
      "(剑鲸秀,719,279,0.3885794)\n",
      "(花鲸补,711,362,0.50985914)\n",
      "(冰花补,700,360,0.51502144)\n",
      "(剑霸歌,698,348,0.49928263)\n",
      "(藏霸补,691,296,0.4289855)\n",
      "(气鲸补,691,284,0.4115942)\n",
      "(冰莫离,689,321,0.46656978)\n",
      "(鲸丐离,683,285,0.41788855)\n",
      "(秃藏补,682,322,0.47283408)\n",
      "(秃螺离,671,321,0.4791045)\n",
      "(苍丐补,669,254,0.38023952)\n",
      "(花毒补,666,348,0.5233083)\n",
      "(花鲸秀,640,330,0.5164319)\n",
      "(苍鲸秀,632,296,0.46909666)\n",
      "(丐霸秀,605,296,0.49006623)\n",
      "(鲸鲸秀,600,285,0.47579297)\n",
      "(气气离,587,346,0.5904437)\n",
      "(莫策秀,584,185,0.3173242)\n",
      "(明花补,578,287,0.49740034)\n",
      "(冰苍秀,562,307,0.5472371)\n",
      "(剑气秀,559,186,0.33333334)\n"
     ]
    }
   ],
   "source": [
    "kungfu_sort_13_result.sortBy(-_._2).take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.DataFrame\n",
      "defined object Helpers\n",
      "import Helpers._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "object Helpers {\n",
    "implicit class DataFrameHist(df:DataFrame) {\n",
    "    def mkHistogram(n_bins: Int) = {\n",
    "        val (startValues, counts) = df.rdd.map(_.getInt(0)).histogram(n_bins)\n",
    "        (startValues.toList, counts.toList)\n",
    "    }\n",
    "}\n",
    "}\n",
    "import Helpers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res10: (List[Double], List[Long]) = (List(2001.0, 2098.6, 2196.2, 2293.8, 2391.4, 2489.0, 2586.6, 2684.2, 2781.8, 2879.4, 2977.0),List(22807, 42267, 43341, 38937, 14724, 2774, 957, 281, 65, 14))\n"
     ]
    }
   ],
   "source": [
    "scores.filter($\"match_type\"===\"3c\").filter($\"score\">2000).select(\"score\").mkHistogram(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ranking_number: org.apache.spark.sql.DataFrame = [ranking: int, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o df_ranking_number\n",
    "val df_ranking_number = scores.filter($\"match_type\"===\"3c\").filter($\"ranking\"<0).groupBy(\"ranking\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d4baf19f0a427ba22bb1aaf5c9f372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7c55bd7bf94e49b2d93a73bc12e6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "df_ranking_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roles_fileterd_10: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 21 more fields]\n",
      "kungfu_weight_10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [kungfu: string, weight: double]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o kungfu_weight_10\n",
    "val roles_fileterd_10 = scores.filter($\"match_type\"===\"3c\").filter(\"score>2100\").join(role_kungfus.withColumnRenamed(\"total_count\", \"kungfu_count\"), Seq(\"role_id\", \"match_type\"))\n",
    "val kungfu_weight_10 = roles_fileterd_10.select(\"role_id\", \"kungfu\", \"kungfu_count\", \"total_count\").withColumn(\"kungfu_weight\", $\"kungfu_count\"/$\"total_count\").groupBy(\"kungfu\").agg(bround(sum(\"kungfu_weight\"), 2).alias(\"weight\")).sort($\"weight\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roles_fileterd_9: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 21 more fields]\n",
      "kungfu_weight_9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [kungfu: string, weight: double]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o kungfu_weight_9\n",
    "val roles_fileterd_9 = scores.filter($\"match_type\"===\"3c\").filter(\"score>2000\").join(role_kungfus.withColumnRenamed(\"total_count\", \"kungfu_count\"), Seq(\"role_id\", \"match_type\"))\n",
    "val kungfu_weight_9 = roles_fileterd_9.select(\"role_id\", \"kungfu\", \"kungfu_count\", \"total_count\").withColumn(\"kungfu_weight\", $\"kungfu_count\"/$\"total_count\").groupBy(\"kungfu\").agg(bround(sum(\"kungfu_weight\"), 2).alias(\"weight\")).sort($\"weight\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roles_fileterd: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 21 more fields]\n",
      "kungfu_weight: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [kungfu: string, weight: double]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o kungfu_weight\n",
    "val roles_fileterd = scores.filter($\"match_type\"===\"3c\").filter(\"score>2400\").join(role_kungfus.withColumnRenamed(\"total_count\", \"kungfu_count\"), Seq(\"role_id\", \"match_type\"))\n",
    "val kungfu_weight = roles_fileterd.select(\"role_id\", \"kungfu\", \"kungfu_count\", \"total_count\").withColumn(\"kungfu_weight\", $\"kungfu_count\"/$\"total_count\").groupBy(\"kungfu\").agg(bround(sum(\"kungfu_weight\"), 2).alias(\"weight\")).sort($\"weight\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roles_fileterd_3000: org.apache.spark.sql.DataFrame = [role_id: string, match_type: string ... 21 more fields]\n",
      "kungfu_weight_3000: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [kungfu: string, weight: double]\n"
     ]
    }
   ],
   "source": [
    "%%spark -o kungfu_weight_3000\n",
    "val roles_fileterd_3000 = scores.filter($\"match_type\"===\"3c\").filter(\"ranking>=0\").join(role_kungfus.withColumnRenamed(\"total_count\", \"kungfu_count\"), Seq(\"role_id\", \"match_type\"))\n",
    "val kungfu_weight_3000 = roles_fileterd_3000.select(\"role_id\", \"kungfu\", \"kungfu_count\", \"total_count\").withColumn(\"kungfu_weight\", $\"kungfu_count\"/$\"total_count\").groupBy(\"kungfu\").agg(bround(sum(\"kungfu_weight\"), 2).alias(\"weight\")).sort($\"weight\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "kungfu_cn_map = {i['content']: i['kungfu'] for i in kungfu_items[[\"content\", \"kungfu\"]].to_dict('records')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14572.620000000003, 95175.95000000001, 112727.84, 2573.19)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "kungfu_weight[\"weight\"].sum(), kungfu_weight_10[\"weight\"].sum(), kungfu_weight_9[\"weight\"].sum(), kungfu_weight_3000[\"weight\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2032d0d8a59c46c7817160786a63a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77e897e01e4f9fbca8d407a9f17a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "kungfu_weight.assign(kungfu_cn=[kungfu_cn_map[x] for x in kungfu_weight['kungfu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5c7523cfe64ade9dd5a22d120bad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626379dbd0f04428abd694c4730fd619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "kungfu_weight_3000.assign(kungfu_cn=[kungfu_cn_map[x] for x in kungfu_weight_3000['kungfu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef2f3d335e7404b88e217d8144752d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7391c13e3146e9bb06336803556e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "kungfu_weight_10.assign(kungfu_cn=[kungfu_cn_map[x] for x in kungfu_weight_10['kungfu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdbba832d6a47949e8baa0d4bf066d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ec8b37f67f48bf9570d8f080720cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "kungfu_weight_9.assign(kungfu_cn=[kungfu_cn_map[x] for x in kungfu_weight_9['kungfu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "sparkkernel"
  },
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
