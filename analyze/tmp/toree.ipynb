{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6ad41d04\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.1.102:4042)\" target=\"new_tab\">Spark UI: app-20180927152527-0001</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180927152527-0001: Some(http://192.168.1.102:4042)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import org.apache.spark.sql.SparkSession\n",
    "// val spark = SparkSession.builder.master(\"spark://127.0.0.1:5737\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%AddDeps org.vegas-viz vegas_2.11 0.3.11 --transitive --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.postgresql:postgresql:42.2.5 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/var/folders/y1/qqvz7wqs5z5_rygrfp97ys5r0000gn/T/toree-tmp-dir6050488973929529089/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /var/folders/y1/qqvz7wqs5z5_rygrfp97ys5r0000gn/T/toree-tmp-dir6050488973929529089/toree_add_deps/https/repo1.maven.org/maven2/org/postgresql/postgresql/42.2.5/postgresql-42.2.5.jar\n",
      "-> New file at /var/folders/y1/qqvz7wqs5z5_rygrfp97ys5r0000gn/T/toree-tmp-dir6050488973929529089/toree_add_deps/https/repo1.maven.org/maven2/org/postgresql/postgresql/42.2.5/postgresql-42.2.5.pom\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.postgresql postgresql 42.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "render = <function1>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function1>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vegas._\n",
    "import vegas.data.External._\n",
    "implicit val render = vegas.render.ShowHTML(kernel.display.content(\"text/html\", _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.8.102:4040)\" target=\"new_tab\">Spark UI: local-1539348716112</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1539348716112: Some(http://192.168.8.102:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(spark.driver.host -> 192.168.8.102, spark.driver.port -> 60660, spark.repl.class.uri -> spark://192.168.8.102:60660/classes, spark.jars -> file:/usr/local/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.2.0-incubating.jar, spark.repl.class.outputDir -> /private/var/folders/y1/qqvz7wqs5z5_rygrfp97ys5r0000gn/T/spark-59a329a2-caef-4075-bad2-4283c02bef74/repl-21ece489-8035-4652-a7eb-35a637c4d8ac, spark.app.name -> Apache Toree, spark.executor.id -> driver, spark.submit.deployMode -> client, spark.master -> local[*], spark.app.id -> local-1539348716112)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.getAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object spark_read_db\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object spark_read_db {\n",
    "    val opts = Map(\"url\" -> \"jdbc:postgresql://localhost:5733/j3\", \"driver\" -> \"org.postgresql.Driver\")\n",
    "    def table(dbtable:String) =\n",
    "        spark.read.format(\"jdbc\").options(opts + (\"dbtable\" -> dbtable))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tag: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|         tag|\n",
      "+------------+\n",
      "|       equip|\n",
      "|metric_names|\n",
      "|      talent|\n",
      "|  attr_names|\n",
      "|     version|\n",
      "|      kungfu|\n",
      "+------------+\n",
      "\n",
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- start_time: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- pvp_type: integer (nullable = true)\n",
      " |-- map: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- team1_score: integer (nullable = true)\n",
      " |-- team2_score: integer (nullable = true)\n",
      " |-- team1_kungfu: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- team2_kungfu: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- winner: integer (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- role_ids: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- role_id: string (nullable = true)\n",
      " |-- match_type: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- score2: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- ranking: integer (nullable = true)\n",
      " |-- ranking2: integer (nullable = true)\n",
      " |-- total_count: integer (nullable = true)\n",
      " |-- win_count: integer (nullable = true)\n",
      " |-- mvp_count: integer (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- fetch_at: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- role_id: integer (nullable = true)\n",
      " |-- global_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- force: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- camp: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- server: string (nullable = true)\n",
      " |-- person_id: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- passport_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- role_id: string (nullable = true)\n",
      " |-- kungfu: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- score2: integer (nullable = true)\n",
      " |-- ranking: integer (nullable = true)\n",
      " |-- equip_score: integer (nullable = true)\n",
      " |-- equip_addition_score: integer (nullable = true)\n",
      " |-- max_hp: integer (nullable = true)\n",
      " |-- metrics: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- equips: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- talents: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- attrs: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- metrics_version: integer (nullable = true)\n",
      " |-- attrs_version: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "items = [tag: string, id: string ... 3 more fields]\n",
       "matches = [match_id: bigint, start_time: int ... 11 more fields]\n",
       "scores = [role_id: string, match_type: string ... 11 more fields]\n",
       "roles = [role_id: int, global_id: string ... 10 more fields]\n",
       "match_roles = [match_id: bigint, role_id: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[match_id: bigint, role_id: string ... 14 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val items = spark_read_db.table(\"items\").load\n",
    "items.printSchema\n",
    "items.select(\"tag\").distinct().show()\n",
    "val matches = spark_read_db.table(\"match_3c.matches\").load\n",
    "matches.printSchema\n",
    "val scores = spark_read_db.table(\"scores\").load\n",
    "scores.printSchema\n",
    "val roles = spark_read_db.table(\"roles\").load\n",
    "roles.printSchema\n",
    "val match_roles = spark_read_db.table(\"match_3c.match_roles\").load\n",
    "match_roles.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----+\n",
      "| content|   id|kungfu|short|\n",
      "+--------+-----+------+-----+\n",
      "|   xisui|10002|    洗髓|    洗|\n",
      "|   yijin|10003|    易经|    秃|\n",
      "|   zixia|10014|    紫霞|    气|\n",
      "|   taixu|10015|    太虚|    剑|\n",
      "| huajian|10021|    花间|    花|\n",
      "|   aoxue|10026|    傲血|    策|\n",
      "|  lijing|10028|    离经|    离|\n",
      "|  tielao|10062|    铁牢|    铁|\n",
      "|yunshang|10080|    云裳|    秀|\n",
      "| bingxin|10081|    冰心|    冰|\n",
      "|cangjian|10145|    藏剑|    藏|\n",
      "|  dujing|10175|    毒经|    毒|\n",
      "|  butian|10176|    补天|    补|\n",
      "|  jingyu|10224|    惊羽|    鲸|\n",
      "| tianluo|10225|    天罗|    螺|\n",
      "| fenying|10242|    焚影|    明|\n",
      "|xiaochen|10268|    笑尘|    丐|\n",
      "|   tiegu|10389|    铁骨|    骨|\n",
      "| fenshan|10390|    分山|    苍|\n",
      "|   mowen|10447|    莫问|    莫|\n",
      "|xiangzhi|10448|    相知|    歌|\n",
      "|   beiao|10464|    北傲|    霸|\n",
      "+--------+-----+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kungfu_cn_map = Map(jingyu -> (惊羽,鲸), yunshang -> (云裳,秀), tiegu -> (铁骨,骨), lijing -> (离经,离), xisui -> (洗髓,洗), fenying -> (焚影,明), xiangzhi -> (相知,歌), taixu -> (太虚,剑), yijin -> (易经,秃), mingzun -> (明尊,喵), cangjian -> (藏剑,藏), tielao -> (铁牢,铁), aoxue -> (傲血,策), fenshan -> (分山,苍), mowen -> (莫问,莫), xiaochen -> (笑尘,丐), butian -> (补天,补), zixia -> (紫霞,气), beiao -> (北傲,霸), bingxin -> (冰心,冰), dujing -> (毒经,毒), tianluo -> (天罗,螺), huajian -> (花间,花))\n",
       "kungfu_items = [content: string, id: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "error: missing or invalid dependency detected while loading class file 'MetadataBuilder.class'.\n",
       "Could not access type AnyRef in package scala,\n",
       "because it (or its dependencies) are missing. Check your build definition for\n",
       "missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.)\n",
       "A full rebuild may help if 'MetadataBuilder.class' was compiled against an incompatible version of scala.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[content: string, id: int ... 2 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kungfu_cn_map = Map(\n",
    "    \"xisui\" -> (\"洗髓\", \"洗\"),\n",
    "    \"yijin\" -> (\"易经\", \"秃\"),\n",
    "    \"zixia\" -> (\"紫霞\", \"气\"),\n",
    "    \"taixu\" -> (\"太虚\", \"剑\"),\n",
    "    \"huajian\" -> (\"花间\", \"花\"),\n",
    "    \"aoxue\" -> (\"傲血\", \"策\"),\n",
    "    \"lijing\" -> (\"离经\", \"离\"),\n",
    "    \"tielao\" -> (\"铁牢\", \"铁\"),\n",
    "    \"yunshang\" -> (\"云裳\", \"秀\"),\n",
    "    \"bingxin\" -> (\"冰心\", \"冰\"),\n",
    "    \"cangjian\" -> (\"藏剑\", \"藏\"),\n",
    "    \"dujing\" -> (\"毒经\", \"毒\"),\n",
    "    \"butian\" -> (\"补天\", \"补\"),\n",
    "    \"jingyu\" -> (\"惊羽\", \"鲸\"),\n",
    "    \"tianluo\" -> (\"天罗\", \"螺\"),\n",
    "    \"fenying\" -> (\"焚影\", \"明\"),\n",
    "    \"mingzun\" -> (\"明尊\", \"喵\"),\n",
    "    \"xiaochen\" -> (\"笑尘\", \"丐\"),\n",
    "    \"tiegu\" -> (\"铁骨\", \"骨\"),\n",
    "    \"fenshan\" -> (\"分山\", \"苍\"),\n",
    "    \"mowen\" -> (\"莫问\", \"莫\"),\n",
    "    \"xiangzhi\" -> (\"相知\", \"歌\"),\n",
    "    \"beiao\" -> (\"北傲\", \"霸\")\n",
    ")\n",
    "val kungfu_items = items.filter(\"tag == 'kungfu'\").\n",
    "    select($\"id\".cast(\"int\"), trim($\"content\", \"\\\"\").alias(\"content\")).\n",
    "    join(kungfu_cn_map.toList.map { case (a, (b, c)) => (a,b,c) }.toDF(\"content\", \"kungfu\", \"short\"), Seq(\"content\"), \"left_outer\")\n",
    "kungfu_items.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|grade|count|duration|\n",
      "+-----+-----+--------+\n",
      "| null|  107|  133.26|\n",
      "|    0|    2|    95.5|\n",
      "|    1|    7|  148.57|\n",
      "|    2|    2|    81.0|\n",
      "|    3|    4|   83.75|\n",
      "|    4|    2|    25.0|\n",
      "|    5|    9|   56.33|\n",
      "|    6|   17|   72.88|\n",
      "|    7|   32|   92.31|\n",
      "|    8|   63|   84.08|\n",
      "|    9|  271|   95.17|\n",
      "|   10|  954|  102.23|\n",
      "|   11| 7436|  115.69|\n",
      "|   12|82572|  130.53|\n",
      "|   13|37923|   158.1|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches.groupBy(\"grade\").agg(count(\"match_id\").alias(\"count\"), bround(avg(\"duration\"), 2).alias(\"duration\")).sort(\"grade\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Helpers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "object Helpers {\n",
    "implicit class DataFrameHist(df:DataFrame) {\n",
    "    def mkHistogram(n_bins: Int) = {\n",
    "        val (startValues, counts) = df.rdd.map(_.getInt(0)).histogram(n_bins)\n",
    "        (startValues.toList, counts.toList)\n",
    "    }\n",
    "}\n",
    "}\n",
    "import Helpers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(List(2.0, 20.06, 38.12, 56.18, 74.24, 92.3, 110.36, 128.42000000000002, 146.48, 164.54, 182.6, 200.66, 218.72, 236.78, 254.84, 272.9, 290.96, 309.02, 327.08, 345.14, 363.2, 381.26, 399.32, 417.38, 435.44, 453.5, 471.56, 489.62, 507.68, 525.74, 543.8, 561.86, 579.92, 597.98, 616.04, 634.1, 652.16, 670.22, 688.28, 706.34, 724.4, 742.46, 760.52, 778.58, 796.64, 814.7, 832.76, 850.82, 868.88, 886.94, 905.0),List(92, 1011, 2903, 4177, 4572, 4181, 3618, 2959, 2207, 1858, 1620, 1338, 1066, 822, 720, 594, 546, 471, 405, 297, 279, 259, 196, 198, 146, 156, 137, 105, 106, 78, 75, 62, 74, 51, 52, 39, 30, 31, 38, 22, 22, 21, 25, 15, 19, 15, 20, 11, 13, 171))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.filter(\"grade==13\").select(\"duration\").mkHistogram(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          duration|\n",
      "+-------+------------------+\n",
      "|  count|             37923|\n",
      "|   mean| 158.0960894444005|\n",
      "| stddev|126.64352824315579|\n",
      "|    min|                 2|\n",
      "|    25%|                80|\n",
      "|    50%|               120|\n",
      "|    75%|               191|\n",
      "|    max|               905|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches.filter(\"grade==13\").select(\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object kungfu_sort_fun\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "get_kungfus: (matches: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "show_kungfu: (kungfus: org.apache.spark.sql.DataFrame, n: Int)scala.collection.mutable.Buffer[(String, Long, Long, Float)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.collection.JavaConverters._\n",
    "def get_kungfus(matches:DataFrame) = {\n",
    "    matches.selectExpr(\"team1_kungfu as kungfu\", \"winner=1 as won\").\n",
    "        union(matches.selectExpr(\"team2_kungfu as kungfu\", \"winner=2 as won\"))\n",
    "}\n",
    "object kungfu_sort_fun {\n",
    "    val kungfu_order = Seq(\n",
    "        \"taixu\",\n",
    "        \"zixia\", \"bingxin\", \"fenying\", \"yijin\", \"huajian\", \"dujing\", \"tianluo\",  \"mowen\",\n",
    "        \"aoxue\", \"fenshan\", \"cangjian\", \"jingyu\", \"xiaochen\", \"beiao\",\n",
    "        \"xisui\", \"tielao\", \"tiegu\", \"mingzun\",\n",
    "        \"lijing\", \"yunshang\", \"butian\", \"xiangzhi\"\n",
    "    ).zipWithIndex.toMap\n",
    "    val kungfu_order_sp = Set((\"jingyu\", \"tianluo\"))\n",
    "    def call(a:String, b:String) : Boolean = {\n",
    "        if (kungfu_order_sp.contains((a, b))) {\n",
    "            return true\n",
    "        }\n",
    "        if (kungfu_order_sp.contains((b, a))) {\n",
    "            return false\n",
    "        }\n",
    "        return kungfu_order(a) < kungfu_order(b)\n",
    "    }\n",
    "}\n",
    "def show_kungfu(kungfus:DataFrame, n:Int) = {\n",
    "    val kungfu_map = kungfu_items.select(\"id\", \"content\").takeAsList(100).asScala.map(i => (i.getAs[Int](0), i.getAs[String](1))).toMap\n",
    "    val kungfu_short_map = kungfu_items.select(\"content\", \"short\").takeAsList(100).asScala.map(i => (i.getAs[String](0), i.getAs[String](1))).toMap\n",
    "    kungfus.takeAsList(n).asScala.map(\n",
    "        i => (\n",
    "            i.getAs[Seq[Int]](0).map(kungfu_map).sortWith(kungfu_sort_fun.call).map(kungfu_short_map).mkString,\n",
    "            i.getAs[Long](1)+1,\n",
    "            i.getAs[Long](2),\n",
    "            i.getAs[Long](2).toFloat/i.getAs[Long](1)\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kungfu_sort = [kungfu: array<int>, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[kungfu: array<int>, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kungfu_sort = get_kungfus(matches).groupBy(\"kungfu\")\n",
    "    .agg(\n",
    "        count(lit(1)).alias(\"count\"),\n",
    "        count(when($\"won\" === true, true)).alias(\"won\")\n",
    "    ).sort($\"won\".desc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(鲸丐秀,11070,6192,0.55940014)\n",
      "(毒毒补,9712,6071,0.6251673)\n",
      "(剑藏秀,10233,5771,0.56401485)\n",
      "(策藏秀,11745,5741,0.48884538)\n",
      "(剑霸秀,7451,4303,0.5775839)\n",
      "(气花离,6959,3774,0.54239726)\n",
      "(明莫补,6211,3588,0.5777778)\n",
      "(秃螺补,4815,2864,0.5949314)\n",
      "(策鲸秀,4833,2477,0.51262414)\n",
      "(剑霸补,4635,2372,0.5118688)\n",
      "(冰毒补,3914,2301,0.5880399)\n",
      "(冰莫歌,3125,2144,0.6862996)\n",
      "(毒莫补,3686,2106,0.5715061)\n",
      "(秃霸补,3540,2013,0.56880474)\n",
      "(气花补,3611,1903,0.5271468)\n",
      "(明莫歌,3028,1862,0.6151305)\n",
      "(剑丐秀,3593,1732,0.48218262)\n",
      "(毒螺补,2793,1578,0.56518626)\n",
      "(冰鲸秀,2593,1558,0.60108024)\n",
      "(苍霸补,2945,1540,0.5230978)\n",
      "(剑策秀,3070,1538,0.5011404)\n",
      "(冰鲸离,2489,1401,0.5631029)\n",
      "(气花秀,2283,1379,0.6042945)\n",
      "(策藏补,3128,1365,0.43652064)\n",
      "(苍藏秀,2624,1358,0.5177278)\n",
      "(剑藏补,2809,1245,0.44337606)\n",
      "(气鲸离,2159,1223,0.5667285)\n",
      "(秃霸秀,2060,1190,0.5779505)\n",
      "(秃秃补,1892,1169,0.6181914)\n",
      "(剑气离,2086,1116,0.5352518)\n",
      "(策霸秀,2930,1079,0.3683851)\n",
      "(明莫离,1930,972,0.503888)\n",
      "(策霸补,2505,971,0.38777956)\n",
      "(秃螺秀,1683,952,0.5659929)\n",
      "(苍霸秀,1921,944,0.49166667)\n",
      "(冰毒秀,1357,896,0.66076696)\n",
      "(藏鲸秀,1570,842,0.53664756)\n",
      "(策鲸补,1718,798,0.46476412)\n",
      "(明莫秀,1291,716,0.55503875)\n",
      "(气花歌,1077,711,0.66078067)\n",
      "(气明秀,1105,687,0.6222826)\n",
      "(气明离,1346,683,0.5078067)\n",
      "(冰莫补,1363,669,0.49118942)\n",
      "(藏霸秀,1268,617,0.4869771)\n",
      "(鲸螺秀,1055,596,0.5654649)\n",
      "(苍藏补,1254,554,0.44213888)\n",
      "(剑丐补,1426,538,0.37754387)\n",
      "(剑策补,1468,527,0.35923654)\n",
      "(秃霸歌,815,508,0.62407863)\n",
      "(气冰离,1168,503,0.43101972)\n",
      "(明毒补,858,503,0.58693117)\n",
      "(气明补,1004,493,0.4915254)\n",
      "(冰花秀,881,492,0.5590909)\n",
      "(明花补,806,482,0.59875774)\n",
      "(秃莫补,969,476,0.49173555)\n",
      "(策苍补,1300,469,0.36104697)\n",
      "(冰花离,980,466,0.47599593)\n",
      "(冰鲸补,1031,462,0.4485437)\n",
      "(气鲸秀,887,448,0.50564337)\n",
      "(冰花补,901,447,0.49666667)\n",
      "(花毒补,761,437,0.575)\n",
      "(策苍秀,1244,433,0.34835076)\n",
      "(冰明补,1021,433,0.4245098)\n",
      "(剑螺秀,841,431,0.51309526)\n",
      "(冰明秀,813,429,0.52832514)\n",
      "(鲸丐补,1096,422,0.38538814)\n",
      "(藏霸补,864,414,0.4797219)\n",
      "(气冰补,890,402,0.45219347)\n",
      "(毒毒秀,703,401,0.57122505)\n",
      "(冰明离,895,397,0.4440716)\n",
      "(冰莫秀,722,396,0.5492372)\n",
      "(明霸补,647,359,0.55572754)\n",
      "(鲸霸秀,638,347,0.544741)\n",
      "(明霸秀,618,340,0.55105346)\n",
      "(冰苍秀,715,332,0.464986)\n",
      "(毒毒离,645,330,0.5124224)\n",
      "(剑苍秀,824,319,0.38760632)\n",
      "(剑霸歌,573,306,0.53496504)\n",
      "(苍丐秀,966,305,0.31606218)\n",
      "(气气离,557,303,0.544964)\n",
      "(气明歌,475,295,0.62236285)\n",
      "(藏鲸补,586,284,0.4854701)\n",
      "(苍鲸秀,592,275,0.46531302)\n",
      "(剑冰秀,551,272,0.49454546)\n",
      "(冰毒离,555,270,0.48736462)\n",
      "(莫鲸秀,457,253,0.5548246)\n",
      "(秃藏补,667,253,0.3798799)\n",
      "(剑螺补,551,249,0.45272726)\n",
      "(剑花离,415,249,0.60144925)\n",
      "(剑苍补,684,248,0.36310396)\n",
      "(鲸鲸秀,477,241,0.50630254)\n",
      "(剑丐离,535,239,0.44756556)\n",
      "(气鲸补,577,233,0.4045139)\n",
      "(冰冰秀,487,229,0.4711934)\n",
      "(秃毒补,443,227,0.51357466)\n",
      "(秃螺离,487,227,0.46707818)\n",
      "(秃花补,416,224,0.53975904)\n",
      "(策丐秀,677,218,0.3224852)\n",
      "(明花离,373,218,0.5860215)\n",
      "(毒苍补,358,214,0.5994398)\n"
     ]
    }
   ],
   "source": [
    "show_kungfu(kungfu_sort, 100).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(List(988.0, 1183.8, 1379.6, 1575.4, 1771.2, 1967.0, 2162.8, 2358.6, 2554.4, 2750.2, 2946.0),List(13, 15, 22, 35, 82, 957, 26943, 19377, 796, 37))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.filter($\"match_type\"===\"3c\").select(\"score\").mkHistogram(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`total`' given input columns: [fetch_at, ranking, total_count, updated_at, match_type, kungfu, score, count, inserted_at, score2, mvp_count, ranking2, role_id, win_count, grade, role_id];;\n",
       "'Project [kungfu#95, count#444L, 'total]\n",
       "+- AnalysisBarrier\n",
       "      +- Join LeftOuter, (role_id#43 = role_id#94)\n",
       "         :- Filter (score#45 > 2400)\n",
       "         :  +- Filter (match_type#44 = 3c)\n",
       "         :     +- Relation[role_id#43,match_type#44,score#45,score2#46,grade#47,ranking#48,ranking2#49,total_count#50,win_count#51,mvp_count#52,inserted_at#53,updated_at#54,fetch_at#55] JDBCRelation(scores) [numPartitions=1]\n",
       "         +- Aggregate [role_id#94, kungfu#95], [role_id#94, kungfu#95, count(1) AS count#444L]\n",
       "            +- Relation[match_id#93L,role_id#94,kungfu#95,score#96,score2#97,ranking#98,equip_score#99,equip_addition_score#100,max_hp#101,metrics#102,equips#103,talents#104,inserted_at#105,attrs#106,metrics_version#107,attrs_version#108] JDBCRelation(match_3c.match_roles) [numPartitions=1]\n",
       "\n",
       "StackTrace: 'Project [kungfu#95, count#444L, 'total]\n",
       "+- AnalysisBarrier\n",
       "      +- Join LeftOuter, (role_id#43 = role_id#94)\n",
       "         :- Filter (score#45 > 2400)\n",
       "         :  +- Filter (match_type#44 = 3c)\n",
       "         :     +- Relation[role_id#43,match_type#44,score#45,score2#46,grade#47,ranking#48,ranking2#49,total_count#50,win_count#51,mvp_count#52,inserted_at#53,updated_at#54,fetch_at#55] JDBCRelation(scores) [numPartitions=1]\n",
       "         +- Aggregate [role_id#94, kungfu#95], [role_id#94, kungfu#95, count(1) AS count#444L]\n",
       "            +- Relation[match_id#93L,role_id#94,kungfu#95,score#96,score2#97,ranking#98,equip_score#99,equip_addition_score#100,max_hp#101,metrics#102,equips#103,talents#104,inserted_at#105,attrs#106,metrics_version#107,attrs_version#108] JDBCRelation(match_3c.match_roles) [numPartitions=1]\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1325)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val role_kungfu = match_roles.groupBy(\"role_id\", \"kungfu\").count()\n",
    "val role_count = role_kungfu.groupBy(\"role_id\").agg(sum(\"count\").alias(\"total\"))\n",
    "val roles_fileterd = scores.filter($\"match_type\"===\"3c\").filter(\"score>2400\")\n",
    "val kungfu_scores = roles_fileterd.join(role_kungfu, roles_fileterd(\"role_id\") === role_kungfu(\"role_id\"), \"left_outer\").select(\"kungfu\", \"count\", \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.execution.debug._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(9) Project [kungfu#95, count#279L, total#259L]\n",
      "+- SortMergeJoin [role_id#43], [role_id#94], LeftOuter\n",
      "   :- *(2) Sort [role_id#43 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(role_id#43, 200)\n",
      "   :     +- *(1) Project [role_id#43]\n",
      "   :        +- *(1) Scan JDBCRelation(scores) [numPartitions=1] [role_id#43] PushedFilters: [*IsNotNull(match_type), *IsNotNull(score), *EqualTo(match_type,3c), *GreaterThan(score,2400)], ReadSchema: struct<role_id:string>\n",
      "   +- *(8) Project [role_id#94, kungfu#95, count#279L, total#259L]\n",
      "      +- *(8) SortMergeJoin [role_id#94], [role_id#284], Inner\n",
      "         :- *(5) Sort [role_id#94 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(role_id#94, 200)\n",
      "         :     +- *(4) HashAggregate(keys=[role_id#94, kungfu#95], functions=[count(1)])\n",
      "         :        +- Exchange hashpartitioning(role_id#94, kungfu#95, 200)\n",
      "         :           +- *(3) HashAggregate(keys=[role_id#94, kungfu#95], functions=[partial_count(1)])\n",
      "         :              +- *(3) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#94,kungfu#95] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string,kungfu:int>\n",
      "         +- *(7) Sort [role_id#284 ASC NULLS FIRST], false, 0\n",
      "            +- *(7) HashAggregate(keys=[role_id#284], functions=[count(1)])\n",
      "               +- Exchange hashpartitioning(role_id#284, 200)\n",
      "                  +- *(6) HashAggregate(keys=[role_id#284], functions=[partial_count(1)])\n",
      "                     +- *(6) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#284] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string>\n"
     ]
    }
   ],
   "source": [
    "kungfu_scores.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results returned: 11118\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 11118\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      " total LongType: {java.lang.Long}\n",
      "== Project [kungfu#95, count#279L, total#259L] ==\n",
      "Tuples output: 0\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      " total LongType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      " total LongType: {}\n",
      "== SortMergeJoin [role_id#43], [role_id#94], LeftOuter ==\n",
      "Tuples output: 11118\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      " total LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 10840\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      "== Sort [role_id#43 ASC NULLS FIRST], false, 0 ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      "== Exchange hashpartitioning(role_id#43, 200) ==\n",
      "Tuples output: 10840\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 10840\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      "== Project [role_id#43] ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      "== Scan JDBCRelation(scores) [numPartitions=1] [role_id#43] PushedFilters: [*IsNotNull(match_type), *IsNotNull(score), *EqualTo(match_type,3c), *GreaterThan(score,2400)], ReadSchema: struct<role_id:string> ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 51485\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      " total LongType: {java.lang.Long}\n",
      "== Project [role_id#94, kungfu#95, count#279L, total#259L] ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      " total LongType: {}\n",
      "== SortMergeJoin [role_id#94], [role_id#284], Inner ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      " role_id StringType: {}\n",
      " total LongType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 51485\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 51485\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== Sort [role_id#94 ASC NULLS FIRST], false, 0 ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      "== Exchange hashpartitioning(role_id#94, 200) ==\n",
      "Tuples output: 52271\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 52271\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== HashAggregate(keys=[role_id#94, kungfu#95], functions=[count(1)]) ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      "== Exchange hashpartitioning(role_id#94, kungfu#95, 200) ==\n",
      "Tuples output: 52271\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 52271\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " kungfu IntegerType: {java.lang.Integer}\n",
      " count LongType: {java.lang.Long}\n",
      "== HashAggregate(keys=[role_id#94, kungfu#95], functions=[partial_count(1)]) ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      " count LongType: {}\n",
      "== Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#94,kungfu#95] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string,kungfu:int> ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " kungfu IntegerType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 50821\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " total LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 50821\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " total LongType: {java.lang.Long}\n",
      "== Sort [role_id#284 ASC NULLS FIRST], false, 0 ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " total LongType: {}\n",
      "== HashAggregate(keys=[role_id#284], functions=[count(1)]) ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " total LongType: {}\n",
      "== InputAdapter ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " count LongType: {}\n",
      "== Exchange hashpartitioning(role_id#284, 200) ==\n",
      "Tuples output: 51451\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " count LongType: {java.lang.Long}\n",
      "== WholeStageCodegen ==\n",
      "Tuples output: 51451\n",
      " role_id StringType: {org.apache.spark.unsafe.types.UTF8String}\n",
      " count LongType: {java.lang.Long}\n",
      "== HashAggregate(keys=[role_id#284], functions=[partial_count(1)]) ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n",
      " count LongType: {}\n",
      "== Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#284] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string> ==\n",
      "Tuples output: 0\n",
      " role_id StringType: {}\n"
     ]
    }
   ],
   "source": [
    "kungfu_scores.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|kungfu|              weight|\n",
      "+------+--------------------+\n",
      "|    冰心|   747.8344895091205|\n",
      "|    太虚|   599.6029102745917|\n",
      "|    洗髓|0.020833333333333332|\n",
      "|    分山|               553.0|\n",
      "|    笑尘|               405.0|\n",
      "|    紫霞|   597.3970897254083|\n",
      "|    莫问|   506.9232297938113|\n",
      "|    傲血|               685.0|\n",
      "|    天罗|   321.1927706165048|\n",
      "|    易经|  335.97916666666663|\n",
      "|    相知|  192.07677020618868|\n",
      "|    北傲|               686.0|\n",
      "|    惊羽|   588.8072293834952|\n",
      "|    离经|   366.5582821330063|\n",
      "|    补天|   928.2281009343933|\n",
      "|    云裳|  1053.1655104908796|\n",
      "|    铁骨|                 1.0|\n",
      "|    花间|  501.44171786699366|\n",
      "|    毒经|  414.77189906560704|\n",
      "|    藏剑|               939.0|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tmp = [kungfu_id: int, weight: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[kungfu_id: int, weight: double]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmp = kungfu_scores.select($\"kungfu\".alias(\"kungfu_id\"), ($\"count\"/$\"total\").alias(\"weight\")).groupBy(\"kungfu_id\").agg(sum(\"weight\").alias(\"weight\")).sort($\"weight\".desc)\n",
    "tmp.join(kungfu_items, tmp(\"kungfu_id\") === kungfu_items(\"id\")).select(\"kungfu\", \"weight\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(11) Sort [weight#1455 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(weight#1455 DESC NULLS LAST, 200)\n",
      "   +- *(10) HashAggregate(keys=[kungfu_id#1448], functions=[sum(weight#1449)])\n",
      "      +- Exchange hashpartitioning(kungfu_id#1448, 200)\n",
      "         +- *(9) HashAggregate(keys=[kungfu_id#1448], functions=[partial_sum(weight#1449)])\n",
      "            +- *(9) Project [kungfu#349 AS kungfu_id#1448, (cast(count#1257L as double) / cast(total#1237L as double)) AS weight#1449]\n",
      "               +- SortMergeJoin [role_id#297], [role_id#348], LeftOuter\n",
      "                  :- *(2) Sort [role_id#297 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(role_id#297, 200)\n",
      "                  :     +- *(1) Project [role_id#297]\n",
      "                  :        +- *(1) Scan JDBCRelation(scores) [numPartitions=1] [role_id#297] PushedFilters: [*IsNotNull(match_type), *IsNotNull(score), *EqualTo(match_type,3c), *GreaterThan(score,2400)], ReadSchema: struct<role_id:string>\n",
      "                  +- *(8) Project [role_id#348, kungfu#349, count#1257L, total#1237L]\n",
      "                     +- *(8) SortMergeJoin [role_id#348], [role_id#1262], Inner\n",
      "                        :- *(5) Sort [role_id#348 ASC NULLS FIRST], false, 0\n",
      "                        :  +- Exchange hashpartitioning(role_id#348, 200)\n",
      "                        :     +- *(4) HashAggregate(keys=[role_id#348, kungfu#349], functions=[count(1)])\n",
      "                        :        +- Exchange hashpartitioning(role_id#348, kungfu#349, 200)\n",
      "                        :           +- *(3) HashAggregate(keys=[role_id#348, kungfu#349], functions=[partial_count(1)])\n",
      "                        :              +- *(3) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#348,kungfu#349] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string,kungfu:int>\n",
      "                        +- *(7) Sort [role_id#1262 ASC NULLS FIRST], false, 0\n",
      "                           +- *(7) HashAggregate(keys=[role_id#1262], functions=[count(1)])\n",
      "                              +- Exchange hashpartitioning(role_id#1262, 200)\n",
      "                                 +- *(6) HashAggregate(keys=[role_id#1262], functions=[partial_count(1)])\n",
      "                                    +- *(6) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#1262] PushedFilters: [*IsNotNull(role_id)], ReadSchema: struct<role_id:string>\n"
     ]
    }
   ],
   "source": [
    "tmp.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "roles_fileterd = [role_id: string, match_type: string ... 11 more fields]\n",
       "role_matches = [role_id: string, match_type: string ... 26 more fields]\n",
       "kungfu_scores = [role_id: string, kungfu: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[role_id: string, kungfu: int ... 2 more fields]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val roles_fileterd = scores.filter($\"match_type\"===\"3c\").filter(\"score>2400\")\n",
    "val role_matches = roles_fileterd.join(match_roles, Seq(\"role_id\"), \"left_outer\")\n",
    "val kungfu_scores =  role_matches.groupBy(\"role_id\", \"kungfu\").count().join(role_matches.groupBy(\"role_id\").count().withColumnRenamed(\"count\", \"total\"), \"role_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(11) Project [role_id#297, kungfu#349, count#1656L, total#1692L]\n",
      "+- *(11) SortMergeJoin [role_id#297], [role_id#1695], Inner\n",
      "   :- *(5) Sort [role_id#297 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(5) HashAggregate(keys=[role_id#297, kungfu#349], functions=[count(1)])\n",
      "   :     +- *(5) HashAggregate(keys=[role_id#297, kungfu#349], functions=[partial_count(1)])\n",
      "   :        +- *(5) Project [role_id#297, kungfu#349]\n",
      "   :           +- SortMergeJoin [role_id#297], [role_id#348], LeftOuter\n",
      "   :              :- *(2) Sort [role_id#297 ASC NULLS FIRST], false, 0\n",
      "   :              :  +- Exchange hashpartitioning(role_id#297, 200)\n",
      "   :              :     +- *(1) Project [role_id#297]\n",
      "   :              :        +- *(1) Scan JDBCRelation(scores) [numPartitions=1] [role_id#297] PushedFilters: [*IsNotNull(match_type), *GreaterThan(score,2400), *EqualTo(match_type,3c), *IsNotNull(role_id), ..., ReadSchema: struct<role_id:string>\n",
      "   :              +- *(4) Sort [role_id#348 ASC NULLS FIRST], false, 0\n",
      "   :                 +- Exchange hashpartitioning(role_id#348, 200)\n",
      "   :                    +- *(3) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#348,kungfu#349] PushedFilters: [], ReadSchema: struct<role_id:string,kungfu:int>\n",
      "   +- *(10) Sort [role_id#1695 ASC NULLS FIRST], false, 0\n",
      "      +- *(10) HashAggregate(keys=[role_id#1695], functions=[count(1)])\n",
      "         +- *(10) HashAggregate(keys=[role_id#1695], functions=[partial_count(1)])\n",
      "            +- *(10) Project [role_id#1695]\n",
      "               +- SortMergeJoin [role_id#1695], [role_id#348], LeftOuter\n",
      "                  :- *(7) Sort [role_id#1695 ASC NULLS FIRST], false, 0\n",
      "                  :  +- ReusedExchange [role_id#1695], Exchange hashpartitioning(role_id#297, 200)\n",
      "                  +- *(9) Sort [role_id#348 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(role_id#348, 200)\n",
      "                        +- *(8) Scan JDBCRelation(match_3c.match_roles) [numPartitions=1] [role_id#348] PushedFilters: [], ReadSchema: struct<role_id:string>\n"
     ]
    }
   ],
   "source": [
    "kungfu_scores.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
